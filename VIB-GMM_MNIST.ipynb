{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.contrib.slim import fully_connected as fc\n",
    "from keras import objectives\n",
    "from sklearn.utils.linear_assignment_ import linear_assignment \n",
    "from sklearn.mixture import GaussianMixture \n",
    "from sklearn.manifold import TSNE\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "from six.moves import cPickle\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {    \n",
    "    'beta_arr': np.linspace(1, 5, 500, dtype='float32'),\n",
    "    'intermediate_dims': [500, 500, 2000],  \n",
    "    'latent_dim': 10,\n",
    "    'batch_size': 100,\n",
    "    'cluster_number': 10 }\n",
    "\n",
    "params['num_epochs'] = len(params['beta_arr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning rate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "    'lr_start': 0.002, # start with lr_start \n",
    "    'lr_decaysteps': 20, # decrease every lr_decaysteps epochs\n",
    "    'lr_decay': 0.9 # with a lr_decay decay rate \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = {\n",
    "    'parameters': params,\n",
    "    'learning_rate_parameters': lr_params,\n",
    "    'acc': [],\n",
    "    'kl_div_loss': [],\n",
    "    'recon_loss': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data set class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \n",
    "    num_batches = 0    \n",
    "    batch_index = 0\n",
    "\n",
    "    def __init__(self, X,  Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "        self.calculate_num_data()\n",
    "        self.calculate_input_dim()\n",
    "   \n",
    "    def calculate_num_data(self):\n",
    "        self.num_data = self.X.shape[0] \n",
    "    \n",
    "    def calculate_input_dim(self):\n",
    "        self.input_dim = self.X.shape[1]\n",
    "        \n",
    "    def calculate_num_batches(self, batch_size):        \n",
    "        self.num_batches = int(np.ceil(self.num_data/batch_size))       \n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        if self.batch_index < (self.num_batches-1):  \n",
    "            idx = range(self.batch_index * batch_size, (self.batch_index + 1) * batch_size)\n",
    "            batch_x = self.X[idx, :]\n",
    "            batch_y = self.Y[idx]\n",
    "            self.batch_index += 1\n",
    "        else:\n",
    "            batch_x = self.X[self.batch_index * batch_size: self.num_data, :]\n",
    "            batch_y = self.Y[self.batch_index * batch_size: self.num_data]\n",
    "            self.batch_index = 0\n",
    "            \n",
    "        return batch_x, batch_y   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cPickle.load(gzip.open('../MNIST_dataset/mnist.pkl.gz', 'rb'), encoding=\"bytes\")\n",
    "\n",
    "# normalize data\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# flatten data\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "# test + train data will be used for unsupervised clusterting\n",
    "X = np.concatenate((x_train,x_test))\n",
    "Y = np.concatenate((y_train,y_test))\n",
    "\n",
    "MNISTdata = Dataset(X, Y)\n",
    "\n",
    "del x_train, x_test, X, Y\n",
    "\n",
    "# calculate number of batches\n",
    "MNISTdata.calculate_num_batches(params['batch_size'])\n",
    "\n",
    "# update parameters\n",
    "params['input_dim'] = MNISTdata.input_dim\n",
    "params['num_data'] = MNISTdata.num_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how to update the learning rate at $n$-th epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_learning_rate(epoch_index):\n",
    "    return max(lr_params['lr_start'] * lr_params['lr_decay']** np.floor(epoch_index / lr_params['lr_decaysteps']), 0.0005) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the global batch index to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_global_batch_index():\n",
    "    sess.run(global_batch_index.assign(0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unsupervised clustering accuracy (ACC) score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsupervised clustering accuracy (ACC)\n",
    "# return accuracy in [0,1]\n",
    "# higher means better accuracy\n",
    "def acc_score(y_true, y_class, num_ground_label):  \n",
    "    \n",
    "    N = y_true.size # number of data \n",
    "    \n",
    "    assert y_class.size == y_true.size # stop if sizes are not equal     \n",
    " \n",
    "    occurance_matrix = np.zeros([num_ground_label, num_ground_label], dtype=np.int64)  \n",
    "    \n",
    "    for i in range(N):\n",
    "        occurance_matrix[y_class[i], y_true[i]] += 1\n",
    "        \n",
    "    # solve the linear assignment problem using the Hungarian algorithm   \n",
    "    # the problem is also known as maximum weight matching in bipartite graphs\n",
    "    # the method is also known as the Munkres or Kuhn-Munkres algorithm \n",
    "    # this makes sure that each cluster assigns to a label \n",
    "    # output = [ [c1,l1], ..., [c10,l10] ]\n",
    "    ind = linear_assignment(-occurance_matrix) \n",
    "\n",
    "    # example: \n",
    "    # occurance_matrix = [ [5, 15, 100], [15, 5, 50], [10, 50, 40] ]\n",
    "    # ind = [ [0, 2], [1, 0], [2, 1]]\n",
    "    # [occurance_matrix[i, j] for i, j in ind] -> [100, 15, 50] # most frequent entires (if possible)\n",
    "\n",
    "    return sum([occurance_matrix[i, j] for i, j in ind]) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  visualisation of the latent space, using t-SNE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space(space, num_samp):\n",
    "        \n",
    "    # colors to be plotted corresponding each label\n",
    "    colors =  ['blue', 'green', 'red', 'cyan', 'magenta', 'orange', 'black', 'yellow', 'pink', 'brown', 'olivedrab']\n",
    "\n",
    "    # transform x to u    \n",
    "    z = sess.run(space, feed_dict={x:MNISTdata.X})\n",
    "\n",
    "    # take a subset of latent space to be visualised\n",
    "    z_subset = z[0:num_samp,:]\n",
    "    label_subset = MNISTdata.Y[0:num_samp]\n",
    "    \n",
    "    print('Features are transformed to the latent space and %i samples are taken to be plotted.' % num_samp)\n",
    "    print('***************************************************************************************************')\n",
    "    \n",
    "    centers = sess.run(gmm_mu)\n",
    "    \n",
    "    label_tsne = np.concatenate((label_subset,10*np.ones(params['cluster_number'])),axis=0).astype(int)\n",
    "    Z = np.concatenate((z_subset,centers),axis=0)\n",
    "\n",
    "    # apply t-SNE algorithm on latent space \n",
    "    x_embedded = TSNE(n_components=2).fit_transform(Z)\n",
    "               \n",
    "    # plot\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(10):\n",
    "        ind = np.where(label_tsne == i)\n",
    "        plt.scatter(x_embedded[ind, 0], x_embedded[ind, 1], c = colors[i] , s=8, label = str(i))\n",
    "\n",
    "    ind = np.where(label_tsne == 10)\n",
    "    plt.scatter(x_embedded[ind, 0], x_embedded[ind, 1], c = colors[10] , s=200, label = 'centers', marker = 'P')    \n",
    "    \n",
    "    plt.legend(bbox_to_anchor = (1.05, 1), loc = 2, borderaxespad = 0, fontsize = 'xx-large', markerscale = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder: $\\mathbf{x} \\rightarrow (\\boldsymbol\\mu_\\mathrm{e}, \\boldsymbol\\Sigma_\\mathrm{e})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x_):    \n",
    "    with tf.variable_scope(name_or_scope = 'encoder', reuse = tf.AUTO_REUSE):\n",
    "        f1 = fc(x_, params['intermediate_dims'][0], activation_fn=tf.nn.relu, scope='HiddenLayer1')  \n",
    "        f2 = fc(f1, params['intermediate_dims'][1], activation_fn=tf.nn.relu, scope='HiddenLayer2')\n",
    "        f3 = fc(f2, params['intermediate_dims'][2], activation_fn=tf.nn.relu, scope='HiddenLayer3')\n",
    "        mu_enc_ = fc(f3, params['latent_dim'], activation_fn=None, scope='LatentLayerMean') \n",
    "        log_sigma_enc_ = fc(f3, params['latent_dim'], activation_fn=None, scope='LatentLayerVariance') \n",
    "    return mu_enc_, log_sigma_enc_  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sampler: $(\\boldsymbol\\mu_\\mathrm{e}, \\boldsymbol\\Sigma_\\mathrm{e}) \\rightarrow \\mathbf{u}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(mu_enc_, log_sigma_enc_):\n",
    "    eps = tf.random_normal(shape=tf.shape(log_sigma_enc_), mean=0, stddev=1, dtype=tf.float32)\n",
    "    u_ = mu_enc_ + tf.exp(log_sigma_enc_ / 2) * eps\n",
    "    return u_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoder: $\\mathbf{u} \\rightarrow \\mathbf{\\hat{x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(u_): \n",
    "    with tf.variable_scope(name_or_scope = 'decoder', reuse = tf.AUTO_REUSE):\n",
    "        g1 = fc(u_, params['intermediate_dims'][-1], activation_fn=tf.nn.relu, scope='HiddenLayer1') \n",
    "        g2 = fc(g1, params['intermediate_dims'][-2], activation_fn=tf.nn.relu, scope='HiddenLayer2')\n",
    "        g3 = fc(g2, params['intermediate_dims'][-3], activation_fn=tf.nn.relu, scope='HiddenLayer3')        \n",
    "        x_hat_ = fc(g3, params['input_dim'], activation_fn=tf.sigmoid, scope='OutputLayer') \n",
    "    return x_hat_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reconstruction loss ( cross entropy between $\\mathbf{x}$ and $\\mathbf{\\hat{x}}$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(x_, x_hat_):    \n",
    "    cross_entropy = params['input_dim'] * objectives.binary_crossentropy(x_, x_hat_)\n",
    "    recon_loss_ = tf.reduce_mean(cross_entropy) \n",
    "    return recon_loss_    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL divergence loss, variational approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLdiv_loss_variational(u_, mu_enc_, log_sigma_enc_, gmm_coef_, gmm_mu_, gmm_sigma_):\n",
    "    \n",
    "    sigma_enc_ = tf.exp(log_sigma_enc_)\n",
    "    \n",
    "    GMM_COEF = tf.tile(tf.expand_dims(gmm_coef_, 0), [params['batch_size'], 1])\n",
    "    GMM_MU = tf.tile(tf.expand_dims(gmm_mu_, 0), [params['batch_size'], 1, 1])\n",
    "    GMM_SIGMA = tf.tile(tf.expand_dims(gmm_sigma_, 0), [params['batch_size'], 1, 1])\n",
    "    \n",
    "    MU_ENC = tf.tile(tf.expand_dims(mu_enc_, 1), [1, params['cluster_number'], 1])\n",
    "    SIGMA_ENC = tf.tile(tf.expand_dims(sigma_enc_, 1), [1, params['cluster_number'], 1])  \n",
    "    \n",
    "    KL = 0.5 * tf.reduce_sum(tf.square(MU_ENC-GMM_MU)/GMM_SIGMA + tf.math.log(GMM_SIGMA/SIGMA_ENC) - 1 + SIGMA_ENC/GMM_SIGMA, axis=2)\n",
    "    \n",
    "    KL_variational = - tf.reduce_logsumexp(tf.math.log(GMM_COEF) - KL, axis=1)\n",
    "    \n",
    "    return tf.reduce_mean(KL_variational)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate assignment probabilities $P_{C|X} = Q_{C|U}$, then do the prediction by taking the argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(u_, gmm_coef_, gmm_mu_, gmm_sigma_):  \n",
    "    \n",
    "    U = tf.tile(tf.expand_dims(u_, 1), [1, params['cluster_number'], 1])\n",
    "    GMM_COEF = tf.tile(tf.expand_dims(gmm_coef_, 0), [params['num_data'], 1])\n",
    "    GMM_MU = tf.tile(tf.expand_dims(gmm_mu_, 0), [params['num_data'], 1, 1])\n",
    "    GMM_SIGMA = tf.tile(tf.expand_dims(gmm_sigma_, 0), [params['num_data'], 1, 1])\n",
    "    \n",
    "    temp1 = tf.math.log(2*np.pi*GMM_SIGMA) + tf.square(U-GMM_MU)/GMM_SIGMA\n",
    "    temp2 = tf.exp( -0.5 * tf.reduce_sum(temp1, axis=2) ) + 1e-10 \n",
    "    q_u_c_ = temp2 * GMM_COEF\n",
    "    \n",
    "    gamma_ = q_u_c_ / tf.reduce_sum(q_u_c_, axis=1, keepdims=True)\n",
    "\n",
    "    return tf.argmax(gamma_, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, beta_curr):  \n",
    "\n",
    "    # initialise epoch index\n",
    "    epoch_index = int( sess.run(global_batch_index) / MNISTdata.num_batches )  \n",
    "    \n",
    "    # loop over epoches \n",
    "    for _ in range(num_epochs):\n",
    "            \n",
    "        # learing rate fot the current epoch      \n",
    "        lr_rate = update_learning_rate(epoch_index)\n",
    "        \n",
    "        # average losses over epoches \n",
    "        recon_loss_sum = 0\n",
    "        kl_div_loss_sum = 0\n",
    "        loss_sum = 0\n",
    "        \n",
    "        # loop over batches  \n",
    "        for _ in range(MNISTdata.num_batches):\n",
    "            \n",
    "            # get the next batch             \n",
    "            batch_x = MNISTdata.next_batch(100)[0]\n",
    "            \n",
    "            # optimize \n",
    "            sess.run(optimizer, feed_dict={x: batch_x, beta: beta_curr, learning_rate: lr_rate})\n",
    "            \n",
    "            # current loss\n",
    "            total_loss_curr, recon_loss_curr, kl_div_loss_curr = sess.run([total_loss, recon_loss, kl_div_loss], feed_dict={x: batch_x, beta: beta_curr})  \n",
    "            \n",
    "            # add the loss for each batch, to be averaged after the batch loop\n",
    "            loss_sum += total_loss_curr            \n",
    "            recon_loss_sum += recon_loss_curr\n",
    "            kl_div_loss_sum += kl_div_loss_curr \n",
    "\n",
    "        # average losses    \n",
    "        loss_av = loss_sum / MNISTdata.num_batches    \n",
    "        recon_loss_av = recon_loss_sum / MNISTdata.num_batches    \n",
    "        kl_div_loss_av = kl_div_loss_sum / MNISTdata.num_batches\n",
    "    \n",
    "        # estimate of y\n",
    "        y_hat_ = sess.run(y_hat, feed_dict={x: MNISTdata.X, learning_rate: lr_rate})\n",
    "        \n",
    "        # accuracy score\n",
    "        acc = acc_score(MNISTdata.Y, y_hat_, params['cluster_number'])          \n",
    "        \n",
    "        # update epoch index\n",
    "        epoch_index = int( sess.run(global_batch_index) / MNISTdata.num_batches )\n",
    "                \n",
    "        # save accuracy score\n",
    "        logs['acc'].append(acc)\n",
    "        logs['kl_div_loss'].append(kl_div_loss_av)\n",
    "        logs['recon_loss'].append(recon_loss_av)\n",
    "\n",
    "        print('-----------------------------------------------------------------------------------------------------------')\n",
    "        print('[Epoch {0: 3d}] Learning Rate: {1:g} \\t Beta: {2:g}'.format(epoch_index, lr_rate, beta_curr)) \n",
    "        print('[Losses averaged over batches] \\t Total Loss: {0:.4f} \\t Reconstruction Loss: {1:.4f} \\t KL Loss: {2:.4f}'.format(loss_av, recon_loss_av, kl_div_loss_av)) \n",
    "        print('ACC score: % {0:.2f}'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, params['input_dim']])\n",
    "learning_rate = tf.placeholder(tf.float32, shape=()) \n",
    "beta = tf.placeholder(tf.float32, shape=()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_coef_init = np.ones(params['cluster_number'], dtype='float32') / params['cluster_number']\n",
    "gmm_mu_init = np.random.randn(params['cluster_number'], params['latent_dim']).astype('float32')    \n",
    "gmm_sigma_init = 0.25*np.abs(np.random.randn(params['cluster_number'], params['latent_dim']).astype('float32'))   \n",
    "\n",
    "with tf.variable_scope(name_or_scope = 'gmm', reuse = False): \n",
    "    gmm_coef = tf.get_variable(name = 'coef', initializer = gmm_coef_init)\n",
    "    gmm_mu = tf.get_variable(name = 'mu', initializer = gmm_mu_init)\n",
    "    gmm_sigma = tf.get_variable(name = 'sigma', initializer = gmm_sigma_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_enc, log_sigma_enc = encoder(x)\n",
    "u = sampler(mu_enc, log_sigma_enc)\n",
    "x_hat = decoder(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_loss = reconstruction_loss(x, x_hat)\n",
    "kl_div_loss = KLdiv_loss_variational(u, mu_enc, log_sigma_enc, gmm_coef, gmm_mu, gmm_sigma) \n",
    "total_loss = recon_loss + beta * kl_div_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = predict(u, gmm_coef, gmm_mu, gmm_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainable variables \n",
    "enc_var = [var for var in tf.trainable_variables() if var.name.startswith('encoder')]\n",
    "dec_var = [var for var in tf.trainable_variables() if var.name.startswith('decoder')]\n",
    "gmm_var = [var for var in tf.trainable_variables() if var.name.startswith('gmm')]\n",
    "\n",
    "# batch counter, which increase at each run of the optimizer\n",
    "global_batch_index = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(total_loss, var_list = enc_var + dec_var + gmm_var, global_step = global_batch_index) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print the model network variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('***************************************************************************************************')  \n",
    "print('Trainable variables:')\n",
    "pprint(enc_var)\n",
    "print('---------------------------------------------------------------------------------------------------')  \n",
    "pprint(dec_var)\n",
    "print('---------------------------------------------------------------------------------------------------')  \n",
    "pprint(gmm_var)\n",
    "print('***************************************************************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### open the sessoin and initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the session\n",
    "sess = tf.Session()\n",
    "\n",
    "# run the initializer\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform x to latent space, x -> u\n",
    "latent_space = sess.run(u, feed_dict={x: MNISTdata.X})\n",
    "\n",
    "# apply EM on latent space \n",
    "gmm =  GaussianMixture(n_components = params['cluster_number'], covariance_type = 'diag', n_init = 2).fit(latent_space)    \n",
    "\n",
    "# initial accuracy score\n",
    "acc = acc_score(MNISTdata.Y, gmm.predict(latent_space), params['cluster_number'])  \n",
    "    \n",
    "print('---------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('Initial Accuracy: ', acc)\n",
    "print('---------------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intialize GMM parameters from pretrained NN weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_op1 = gmm_coef.assign(gmm.weights_)\n",
    "assign_op2 = gmm_mu.assign(gmm.means_)\n",
    "assign_op3 = gmm_sigma.assign(gmm.covariances_)    \n",
    "sess.run([assign_op1, assign_op2, assign_op3]) \n",
    "\n",
    "print('---------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('GMM parameters are initialized! (VAE)')\n",
    "print('---------------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display initial GMM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('Initial GMM coefficient:')\n",
    "print(sess.run(gmm_coef))\n",
    "print('---------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('Initial GMM mean:')\n",
    "print(sess.run(gmm_mu))\n",
    "print('---------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('Initial GMM variance:')\n",
    "print(sess.run(gmm_sigma))\n",
    "print('---------------------------------------------------------------------------------------------------------------------------------')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(params['num_epochs']):    \n",
    "    train(num_epochs = 1, beta_curr = params['beta_arr'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dataset_name = 'MNIST_VIB-GMM'\n",
    "    \n",
    "current_time = '_' + datetime.now().strftime('%b%d_%H%M%S')\n",
    "\n",
    "f_nn_name = '_NN'\n",
    "for i in range(np.shape(params['intermediate_dims'])[0]):\n",
    "    f_nn_name = f_nn_name + '_' + str(params['intermediate_dims'][i])  \n",
    "f_nn_name = f_nn_name + '_' + str(params['latent_dim']) \n",
    "        \n",
    "f_name = f_dataset_name + f_nn_name + current_time       \n",
    "    \n",
    "# save to pickle file\n",
    "with open('logs/'+f_name+'.pickle', 'wb') as f:\n",
    "    pickle.dump(logs, f) \n",
    "print('***************************************************************************************************') \n",
    "print('Logs stored in the directory: logs/', f_name)    \n",
    "print('***************************************************************************************************')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  visualisation of the latent space, using t-SNE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space(u, num_samp = 2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
